#include <csvm/csvm_rbm.h>
//DEPRECATED

using namespace std;
using namespace csvm;

   union charInt{
      unsigned char chars[4];
      int intVal;
   };
   
   union charfloat{
      char chars[8];
      float floatVal;
   };
   
   RBM::RBM(){
      settings.nLayers = 2;
      settings.layerSizes = (int*) malloc(2*sizeof(int));
      assert(settings.layerSizes!=NULL);
      settings.layerSizes[0] = 300;
      settings.layerSizes[1] = 100;
      settings.learningRate = 0.01;
      settings.nGibbsSteps = 1;
      
      initLayerStack(&layers,settings.nLayers,settings.layerSizes);
      initStackWeightsRandom(&layers);
   }
   
   RBM::RBM(int nLayers,int* layerSizes,float learningRate){
      initLayerStack(&layers,nLayers,layerSizes);
      this->settings.learningRate = learningRate;
      this->settings.layerSizes = layerSizes;
      this->settings.nLayers = nLayers;
   }
   
   
   RBM::RBM(int nLayers,int* layerSizes,float learningRate,float** dataset,int nDataEntries){
      initLayerStack(&layers,nLayers,layerSizes);
      data.data = dataset;
      data.size = nDataEntries;
      this->settings.learningRate = learningRate;
      this->settings.layerSizes = layerSizes;
      this->settings.nLayers = nLayers;
   }
   
   RBM::~RBM(){
      freeLayerStack(&layers);
      free(settings.layerSizes);
      
   }
   
   void RBM::setSettings(RBMSettings set){
      this->settings = set;
   }
   
   
   void RBM::linkDataset(float** data,int nEntries){
      this->data.data = data; 
      this->data.size = nEntries;
     
   }
   
   void RBM::freeDataset(){
      unsigned int size = data.size;
      for(size_t idx = 0; idx < size; ++idx)
         free(data.data[idx]);
      free(data.data);
   }
   
   void RBM::train(){
      cout << "Training rbm..\n";
      performRBM(&layers,&data,settings.learningRate,0,settings.nGibbsSteps);
   }
   
   float* RBM::getOutput(){
      float* output = new float[settings.layerSizes[settings.nLayers-1]];
      for (int i = 0; i < settings.layerSizes[settings.nLayers - 1];i++){
         output[i] = layers.layers[settings.nLayers-1][i];
      }
      return output;
   }
   
   float* RBM::run(float* input){
      for(int i = 0;i < settings.layerSizes[0];i++){
         layers.layers[0][i] = input[i];
      }
      flowUp(&layers);
      return getOutput();
   }
   
   void RBM::exportNetwork(string filename){
      /* Format:
      * 4 bytes: int : number of layers
      * for each of layer: 4 bytes int layerSize;
      * 
      *
      */
      charfloat fancyfloat;
      charInt fancyInt;
      
      ofstream file(filename.c_str(), ios::out | ios::binary);
      
      
      //write number of layers
      fancyInt.intVal = settings.nLayers;
      for(size_t idx = 0; idx < 4; ++idx){
         file << fancyInt.chars[idx];      
      }
      //write layer sizes
      for(size_t layer = 0; layer < (unsigned int)settings.nLayers; ++layer){
         fancyInt.intVal = settings.layerSizes[layer];
         for(size_t idx = 0; idx < 4; ++idx){
            file << fancyInt.chars[idx];    
         }
      }
      //write weights 
      
      for(size_t layer = 0; layer < (unsigned int)settings.nLayers - 1; ++layer){
         for(size_t lIdx1 = 0; lIdx1 < (unsigned int) settings.layerSizes[layer]; ++lIdx1){  //source neuron index
            for(size_t lIdx2 = 0; lIdx2 < (unsigned int)settings.layerSizes[layer]; ++lIdx2){  //target neuron
               fancyfloat.floatVal = layers.weights[layer][lIdx1][lIdx2];
               for(size_t idxt = 0; idxt < 8; ++ idxt)
                  file << fancyfloat.chars[idxt];
            }
         }
      }
      
      file.close();
         
   }
   
   void RBM::importNetwork(string filename){
      charfloat fancyfloat;
      charInt fancyInt;
      
      ifstream file(filename.c_str(), ios::in | ios::binary);
      
      
      //free weights in advance since we still have info about it
      for(size_t layer = 0; layer < (unsigned int)(layers.nLayers); ++layer){
         for(size_t neuron = 0; neuron <  (unsigned int)(layers.layerSizes[layer]); ++neuron)
            free(layers.weights[layer][neuron]);
         free(layers.weights[layer]);
      }
      free(layers.weights);
      
      //read int4 number of layers
      for(size_t idx = 0; idx < 4; ++idx)
         file >> fancyInt.chars[idx];
      settings.nLayers = fancyInt.intVal;
      layers.nLayers = settings.nLayers;
      
      //allocate layerSizes;
      free(layers.layerSizes);
      free(settings.layerSizes);
      layers.layerSizes = (int*) malloc(layers.nLayers * sizeof(int));
      assert(layers.layerSizes != NULL);
      settings.layerSizes = layers.layerSizes;  
      
      //read layerSizes
      for(size_t layer = 0; layer <  (unsigned int)(layers.nLayers); ++layer){
         for(size_t idx = 0; idx < 4; ++idx)
            file >> fancyInt.chars[idx];
         layers.layerSizes[layer] = fancyInt.intVal;
      }
      
      //allocate layers
      for(size_t layer = 0; layer <  (unsigned int)(layers.nLayers); ++layer)
         free(layers.layers[layer]);
      free(layers.layers);
      layers.layers = (float**) malloc(layers.nLayers * sizeof(float*));
      assert(layers.layers != NULL);
      
      
      for(size_t layer = 0; layer <  (unsigned int)(layers.nLayers); ++layer){
         layers.layers[layer] = (float*) malloc(layers.layerSizes[layer] * sizeof(float));
         assert(layers.layers[layer] != NULL);
      }
      
      //allocate weights
      layers.weights = (float***) malloc ((layers.nLayers - 1)* sizeof(float**));
      assert(layers.weights != NULL);
      for(size_t layer = 0; layer <  (unsigned int)(layers.nLayers - 1); ++layer){
         for(size_t idxL1 = 0; idxL1 <  (unsigned int)(layers.layerSizes[layer]); ++idxL1){
            layers.weights[layer] = (float**) malloc(layers.layerSizes[layer] * sizeof(float*));
            assert(layers.weights[layer] != NULL); 
            for(size_t idxL2 = 0; idxL2 < (unsigned int)( layers.layerSizes[layer + 1]); ++idxL2){
               layers.weights[layer][idxL1] = (float*) malloc(layers.layerSizes[layer + 1] * sizeof(float));
               assert(layers.weights[layer][idxL1] != NULL);
               for(size_t weight = 0; weight <  (unsigned int)(layers.layerSizes[layer + 1]); ++weight){
                  for(size_t idxfloat = 0; idxfloat < 4; ++ idxfloat){
                     file >> fancyfloat.chars[idxfloat];
                  }
                  layers.weights[layer][idxL1][idxL2] = fancyfloat.floatVal;
               }
            }
         }
      }
      
      
      
   }